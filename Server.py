# Archivo: server.py
# Este script levanta un servidor Flask que expone el modelo Llama 3 (a trav√©s de Ollama)
# Permite el acceso remoto y mantiene el historial de chat para cada usuario.

import os
import time
from flask import Flask, request, jsonify
from flask_cors import CORS
from ollama import Client # ‚¨ÖÔ∏è CORRECCI√ìN: Quitamos OllamaBaseException para evitar el ImportError.

# --- CONFIGURACI√ìN DE SEGURIDAD Y MODELO ---
MODELO_LLM = "llama3:8b" 
API_KEY_SECRETA = "MiClaveSuperSegura123" # ‚¨ÖÔ∏è IMPORTANTE: ¬°Cambia esto por una clave m√°s fuerte!
# ------------------------------------------

# --- Inicializaci√≥n del Cliente Ollama ---
try:
    # Se conecta al servicio Ollama que debe estar corriendo localmente en el puerto 11434
    ollama_client = Client()
    ollama_client.list() 
    print("‚úÖ Conexi√≥n con Ollama establecida correctamente.")
except Exception as e: # ‚¨ÖÔ∏è Usamos Exception gen√©rico para mayor compatibilidad.
    # Si la lista falla, asumimos que el servicio Ollama no est√° activo.
    print(f"‚ùå ERROR CR√çTICO: No se pudo conectar al servicio de Ollama en localhost:11434.")
    print(f"Detalles del error: {e}")
    print("Aseg√∫rate de que el servidor 'ollama run llama3:8b' est√© activo ANTES de iniciar Flask.")
    exit(1)


# --- Configuraci√≥n de Flask y Almacenamiento de Historial ---
app = Flask(__name__)
CORS(app) # Habilita CORS para permitir acceso desde otros dominios (navegadores)

# Diccionario para almacenar el historial de chat de cada sesi√≥n.
# La clave es el 'session_id' enviado por el cliente.
chat_history = {}


# --- Ruta de Chat Principal ---

@app.route("/chat", methods=["POST"])
def chat():
    # 1. Verificaci√≥n de la Clave API (Seguridad)
    cliente_api_key = request.headers.get("X-API-Key")
    if cliente_api_key != API_KEY_SECRETA:
        return jsonify({"response": "Acceso denegado. Clave API incorrecta."}), 401

    # 2. Extracci√≥n y validaci√≥n de datos
    data = request.get_json()
    mensaje = data.get("mensaje", "")
    session_id = data.get("session_id", "default_session")

    if not mensaje or not mensaje.strip():
        return jsonify({"response": "No enviaste ning√∫n mensaje."}), 400

    print(f"[{time.strftime('%H:%M:%S')}] Recibido (Session {session_id}): '{mensaje[:50]}...'")

    # 3. Gesti√≥n del historial
    if session_id not in chat_history:
        # Inicializa el historial para la nueva sesi√≥n
        chat_history[session_id] = []

    # Agrega el mensaje actual del usuario al historial
    chat_history[session_id].append({"role": "user", "content": mensaje})

    # 4. Llamada al modelo Ollama con el historial completo
    try:
        response = ollama_client.chat(
            model=MODELO_LLM,
            messages=chat_history[session_id], # Pasa toda la conversaci√≥n para contexto
            options={
                "temperature": 0.8
            }
        )
        
        salida = response['message']['content']

        # 5. Agrega la respuesta del asistente al historial
        chat_history[session_id].append({"role": "assistant", "content": salida})


    except Exception as e: # ‚¨ÖÔ∏è Usamos Exception gen√©rico para manejar cualquier fallo del LLM
        print(f"Error al ejecutar el modelo Ollama: {e}")
        # Retira el √∫ltimo mensaje de usuario si la llamada falla
        chat_history[session_id].pop() 
        return jsonify({"response": f"Error del modelo Ollama: {e}"}), 500

    return jsonify({"response": salida})

if __name__ == "__main__":
    print("-" * 50)
    print(f"üî• Servidor de Ollama (Modelo: {MODELO_LLM})")
    print(f"üîë API Key: {API_KEY_SECRETA}")
    print("üåê Direcci√≥n local: http://0.0.0.0:5000")
    print("-" * 50)
    
    # Escucha en 0.0.0.0 (todas las interfaces) para que ngrok lo pueda alcanzar
    app.run(host="0.0.0.0", port=5000)
